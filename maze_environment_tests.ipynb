{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jumanji\n",
    "import chex\n",
    "\n",
    "\n",
    "env = jumanji.environments.Maze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(agent_position=Position(row=Array(0, dtype=int32), col=Array(2, dtype=int32)), target_position=Position(row=Array(1, dtype=int32), col=Array(4, dtype=int32)), walls=Array([[False, False, False, False, False,  True, False, False, False,\n",
      "        False],\n",
      "       [False,  True, False,  True, False,  True, False,  True, False,\n",
      "         True],\n",
      "       [False,  True, False,  True, False,  True, False,  True, False,\n",
      "        False],\n",
      "       [False,  True,  True,  True, False,  True, False,  True,  True,\n",
      "         True],\n",
      "       [False,  True, False, False, False, False, False,  True, False,\n",
      "         True],\n",
      "       [False,  True, False,  True, False,  True, False,  True, False,\n",
      "         True],\n",
      "       [False,  True, False,  True, False,  True, False, False, False,\n",
      "        False],\n",
      "       [False,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "         True],\n",
      "       [False,  True, False, False, False, False, False, False, False,\n",
      "        False],\n",
      "       [False,  True, False,  True, False,  True, False,  True, False,\n",
      "         True]], dtype=bool), action_mask=Array([False,  True,  True,  True], dtype=bool), step_count=Array(0, dtype=int32), key=Array([1325352869, 2119862813], dtype=uint32))\n"
     ]
    }
   ],
   "source": [
    "key=jax.random.PRNGKey(10)\n",
    "state, timestep = env.reset(key)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position(row=Array(1, dtype=int32), col=Array(4, dtype=int32))\n",
      "Position(row=Array(0, dtype=int32), col=Array(2, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "target_position = state['target_position']\n",
    "agent_position = state['agent_position']\n",
    "print(target_position)\n",
    "print(agent_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(target_position[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, timestep = env.step(state, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position(row=Array(0, dtype=int32), col=Array(3, dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print(new_state['agent_position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=Array(0, dtype=int8), reward=Array(0., dtype=float32), discount=Array(1., dtype=float32), observation=Observation(agent_position=Position(row=Array(0, dtype=int32), col=Array(2, dtype=int32)), target_position=Position(row=Array(1, dtype=int32), col=Array(4, dtype=int32)), walls=Array([[False, False, False, False, False,  True, False, False, False,\n",
      "        False],\n",
      "       [False,  True, False,  True, False,  True, False,  True, False,\n",
      "         True],\n",
      "       [False,  True, False,  True, False,  True, False,  True, False,\n",
      "        False],\n",
      "       [False,  True,  True,  True, False,  True, False,  True,  True,\n",
      "         True],\n",
      "       [False,  True, False, False, False, False, False,  True, False,\n",
      "         True],\n",
      "       [False,  True, False,  True, False,  True, False,  True, False,\n",
      "         True],\n",
      "       [False,  True, False,  True, False,  True, False, False, False,\n",
      "        False],\n",
      "       [False,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "         True],\n",
      "       [False,  True, False, False, False, False, False, False, False,\n",
      "        False],\n",
      "       [False,  True, False,  True, False,  True, False,  True, False,\n",
      "         True]], dtype=bool), action_mask=Array([False,  True,  True,  True], dtype=bool), step_count=Array(0, dtype=int32)), extras={})\n"
     ]
    }
   ],
   "source": [
    "print(timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(timestep['reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maze environment:\n",
      " - num_rows: 10\n",
      " - num_cols: 10\n",
      " - time_limit: 100\n",
      " - generator: <jumanji.environments.routing.maze.generator.RandomGenerator object at 0x000002CD856C9DC0>\n"
     ]
    }
   ],
   "source": [
    "print(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "walls = new_state['walls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[90]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "state_indices = jax.random.choice(key, jnp.arange(env.num_rows * env.num_cols), (1, ), replace=False, p=~walls.flatten())\n",
    "print(state_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0\n"
     ]
    }
   ],
   "source": [
    "(state_row, state_col) = jnp.divmod(state_indices, env.num_cols)\n",
    "print(state_row[0], state_col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jumanji.environments.routing.maze.types import Position, State\n",
    "\n",
    "\n",
    "def rollin_mdp(env, rollin_type, optimal_actions, seed = 10):\n",
    "    states = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "    rewards = []\n",
    "\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "\n",
    "    state, timestep = env.reset(key)\n",
    "\n",
    "    goal_state = state['target_position']\n",
    "    walls = state['walls']\n",
    "    maze_key = state['key']\n",
    "\n",
    "    for i in range(env.time_limit):\n",
    "        if rollin_type == 'uniform':\n",
    "            state = sample_state(env, walls, goal_state, maze_key, i)\n",
    "            action = sample_action()  \n",
    "        elif rollin_type == 'expert':\n",
    "            action = optimal_actions[state] \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        next_state, timestep = env.step(state, action)\n",
    "        reward = timestep['reward']\n",
    "\n",
    "\n",
    "        agent_position = state['agent_position']  # TODO this is a Position type. which kind of data structure do we want? (E.G. array/list)\n",
    "        next_agent_position = next_state['agent_position']\n",
    "\n",
    "        states.append([agent_position[0], agent_position[1]])\n",
    "        actions.append(action)\n",
    "        next_states.append([next_agent_position[0], next_agent_position[1]])\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    next_states = np.array(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "\n",
    "    return states, actions, next_states, rewards, goal_state, walls\n",
    "\n",
    "def find_optimal_actions(env): #TODO\n",
    "\n",
    "    #it should be a dictionary where the keys are 'Position' types and the values are the corresponding actions\n",
    "\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    return [[sample_action() for j in range(env.num_cols)] for i in range(env.num_rows)]\n",
    "\n",
    "def sample_state(env, walls, target_position = None, maze_key = None, step_count = None):\n",
    "\n",
    "    seed = np.random.randint(low = 0, high = env.num_rows*env.num_cols)\n",
    "\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    state_indices = jax.random.choice(key, jnp.arange(env.num_rows * env.num_cols), (1,), replace=False, p=~walls.flatten())\n",
    "    (state_row, state_col) = jnp.divmod(state_indices, env.num_cols)\n",
    "\n",
    "    agent_position = Position(row = state_row[0], col = state_col[0])\n",
    "\n",
    "    if target_position is None:\n",
    "        return agent_position #in this case we only want to return the agent position and not the full state\n",
    "    else:\n",
    "        return State(agent_position=agent_position, target_position=target_position, walls=walls, action_mask=jnp.array([True, True, True, True]), key=maze_key, step_count=jnp.array(step_count+1, jnp.int32))\n",
    "\n",
    "\n",
    "def sample_action():\n",
    "\n",
    "    return np.random.choice([1, 2, 3, 4])\n",
    "\n",
    "def generate_maze_histories_from_envs(envs, n_hists, n_samples, rollin_type):\n",
    "    trajs = []\n",
    "    for env in envs:\n",
    "\n",
    "        optimal_actions = find_optimal_actions(env) #TODO: since in generate_mdp_histories the optimal actions are required and usually they are part\n",
    "\n",
    "        for j in range(n_hists):\n",
    "            (context_states, context_actions, context_next_states, context_rewards, goal_state, walls) = rollin_mdp(env, rollin_type=rollin_type, optimal_actions=optimal_actions)\n",
    "            \n",
    "            for k in range(n_samples):\n",
    "                query_state = sample_state(env, walls) \n",
    "                print(query_state)\n",
    "                optimal_action = optimal_actions(query_state)\n",
    "\n",
    "                traj = {\n",
    "                    'query_state': query_state,\n",
    "                    'optimal_action': optimal_action,\n",
    "                    'context_states': context_states,\n",
    "                    'context_actions': context_actions,\n",
    "                    'context_next_states': context_next_states,\n",
    "                    'context_rewards': context_rewards,\n",
    "                    'goal': goal_state,\n",
    "                }\n",
    "\n",
    "                trajs.append(traj)\n",
    "    return trajs\n",
    "\n",
    "    \n",
    "def generate_maze_histories(horizon, n_envs, **kwargs):\n",
    "\n",
    "    envs = [jumanji.environments.Maze(time_limit = horizon) for _ in range(n_envs)]\n",
    "    trajs = generate_maze_histories_from_envs(envs, **kwargs)\n",
    "                                              \n",
    "    return trajs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_actions = find_optimal_actions(env)\n",
    "states, actions, next_states, rewards, goal_state, walls = rollin_mdp(env, 'uniform', optimal_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4]\n",
      " [3 0]\n",
      " [0 7]\n",
      " [9 2]\n",
      " [8 2]\n",
      " [9 0]\n",
      " [3 0]\n",
      " [2 4]\n",
      " [1 6]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [5 4]\n",
      " [8 8]\n",
      " [1 8]\n",
      " [4 5]\n",
      " [8 7]\n",
      " [4 2]\n",
      " [9 8]\n",
      " [3 4]\n",
      " [6 6]\n",
      " [4 6]\n",
      " [0 7]\n",
      " [9 2]\n",
      " [8 9]\n",
      " [9 0]\n",
      " [9 0]\n",
      " [9 2]\n",
      " [2 8]\n",
      " [8 8]\n",
      " [0 7]\n",
      " [4 2]\n",
      " [4 0]\n",
      " [8 5]\n",
      " [6 2]\n",
      " [2 2]\n",
      " [8 5]\n",
      " [0 8]\n",
      " [4 8]\n",
      " [4 2]\n",
      " [8 2]\n",
      " [8 8]\n",
      " [8 5]\n",
      " [8 8]\n",
      " [2 4]\n",
      " [9 0]\n",
      " [2 2]\n",
      " [8 2]\n",
      " [6 0]\n",
      " [2 8]\n",
      " [6 6]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [1 8]\n",
      " [3 4]\n",
      " [4 0]\n",
      " [6 6]\n",
      " [4 2]\n",
      " [6 6]\n",
      " [6 2]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [5 8]\n",
      " [5 2]\n",
      " [1 8]\n",
      " [9 2]\n",
      " [0 7]\n",
      " [9 2]\n",
      " [2 9]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [4 2]\n",
      " [8 2]\n",
      " [2 0]\n",
      " [5 2]\n",
      " [0 7]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [4 5]\n",
      " [2 8]\n",
      " [4 8]\n",
      " [9 2]\n",
      " [4 8]\n",
      " [9 0]\n",
      " [1 2]\n",
      " [3 4]\n",
      " [1 8]\n",
      " [4 0]\n",
      " [0 7]\n",
      " [4 0]\n",
      " [2 8]\n",
      " [6 0]\n",
      " [4 4]\n",
      " [1 8]\n",
      " [9 8]\n",
      " [4 5]\n",
      " [6 6]\n",
      " [6 2]\n",
      " [0 7]]\n"
     ]
    }
   ],
   "source": [
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trajectories \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_maze_histories\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_hists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrollin_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muniform\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(trajectories)\n",
      "Cell \u001b[1;32mIn[98], line 106\u001b[0m, in \u001b[0;36mgenerate_maze_histories\u001b[1;34m(horizon, n_envs, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_maze_histories\u001b[39m(horizon, n_envs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    105\u001b[0m     envs \u001b[38;5;241m=\u001b[39m [jumanji\u001b[38;5;241m.\u001b[39menvironments\u001b[38;5;241m.\u001b[39mMaze(time_limit \u001b[38;5;241m=\u001b[39m horizon) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_envs)]\n\u001b[1;32m--> 106\u001b[0m     trajs \u001b[38;5;241m=\u001b[39m generate_maze_histories_from_envs(envs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trajs\n",
      "Cell \u001b[1;32mIn[98], line 87\u001b[0m, in \u001b[0;36mgenerate_maze_histories_from_envs\u001b[1;34m(envs, n_hists, n_samples, rollin_type)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m     86\u001b[0m     query_state \u001b[38;5;241m=\u001b[39m sample_state(env, walls) \n\u001b[1;32m---> 87\u001b[0m     optimal_action \u001b[38;5;241m=\u001b[39m \u001b[43moptimal_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     traj \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_state\u001b[39m\u001b[38;5;124m'\u001b[39m: query_state,\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimal_action\u001b[39m\u001b[38;5;124m'\u001b[39m: optimal_action,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoal\u001b[39m\u001b[38;5;124m'\u001b[39m: goal_state,\n\u001b[0;32m     97\u001b[0m     }\n\u001b[0;32m     99\u001b[0m     trajs\u001b[38;5;241m.\u001b[39mappend(traj)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "trajectories = generate_maze_histories(10, 1, n_hists=1, n_samples=5, rollin_type = 'uniform')\n",
    "print(trajectories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
